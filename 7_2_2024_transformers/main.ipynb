{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from data import ParenthesizationDataset\n",
    "from model import Model\n",
    "from train import train_one_epoch, compute_validation_loss\n",
    "from evaluate import evaluate_model, predict\n",
    "from interpret import plot_linear_layer, incorrect_predictions, token_contributions, activations\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model, loaders, loss function, optimizer\n",
    "`checkpoint_epochs` is the number of epochs before a checkpoint is saved. Use `data_prefix` to choose the \"small\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "checkpoint_epochs = 2\n",
    "batch_size = 64\n",
    "d_model = 16\n",
    "nhead = 4\n",
    "num_layers = 1\n",
    "n_vocab = 4\n",
    "data_prefix = \"small_\"\n",
    "\n",
    "training_dataset = ParenthesizationDataset(f\"{data_prefix}training\")\n",
    "validation_dataset = ParenthesizationDataset(f\"{data_prefix}validation\")\n",
    "test_dataset = ParenthesizationDataset(f\"{data_prefix}test\")\n",
    "\n",
    "training_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = Model(n_vocab, d_model, nhead, num_layers)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "`start_epoch` can be set to a value greater than 0 to resume training from a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "best_validation_loss = float(\"inf\")\n",
    "\n",
    "def save():\n",
    "    torch.save({\n",
    "      'model_state_dict': model.state_dict(),\n",
    "      'optimizer_state_dict': optimizer.state_dict(),\n",
    "      'training_loss': training_loss,\n",
    "      'validation_loss': validation_loss\n",
    "    }, f\"checkpoints/epoch_{epoch}.pth\")\n",
    "\n",
    "def resume(epoch):\n",
    "    global training_loss, validation_loss\n",
    "    checkpoint = torch.load(f\"checkpoints/epoch_{epoch}.pth\")\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    training_loss = checkpoint[\"training_loss\"]\n",
    "    validation_loss = checkpoint[\"validation_loss\"]\n",
    "\n",
    "if start_epoch > 0:\n",
    "    resume(start_epoch)\n",
    "    start_epoch += 1\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    model.train(True)\n",
    "    training_loss.append(train_one_epoch(training_loader, model, loss_fn, optimizer))\n",
    "    validation_loss.append(compute_validation_loss(validation_loader, model, loss_fn))\n",
    "    print(f\"Epoch {epoch}, Training Loss {training_loss[-1]}, Validation Loss {validation_loss[-1]}\")\n",
    "\n",
    "    if (epoch+1) % checkpoint_epochs == 0:\n",
    "        save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the loss curve for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss function\n",
    "plt.plot(training_loss, label=\"Training loss\")\n",
    "plt.plot(validation_loss, label=\"Validation loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model and plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_confusion_matrix = evaluate_model(model, training_loader)\n",
    "validation_confusion_matrix = evaluate_model(model, validation_loader)\n",
    "test_confusion_matrix = evaluate_model(model, test_loader)\n",
    "\n",
    "print(\"Training data set accuracy:\")\n",
    "training_confusion_matrix.print_accuracy()\n",
    "training_confusion_matrix.plot()\n",
    "\n",
    "print(\"Validation data set accuracy:\")\n",
    "validation_confusion_matrix.print_accuracy()\n",
    "validation_confusion_matrix.plot()\n",
    "\n",
    "print(\"Test data set accuracy:\")\n",
    "test_confusion_matrix.print_accuracy()\n",
    "test_confusion_matrix.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "In this section you will try various techniques to interpret the model. Insert additional notebook cells as need to do the following:\n",
    "\n",
    "1. Construct some test cases of varying lengths and display the model's predictions.\n",
    "2. Compute the list of incorrect predictions and display some examples of them.\n",
    "3. For each incorrect prediction, plot the token contributions as a heatmap.\n",
    "4. For each position from 0 to 21, loop over all inputs in the test set and plot a histogram of the token contributions for that position.\n",
    "5. Plot the heatmap for the projection layer and the second linear layer in the feedforward.\n",
    "6. Loop over all inputs in the test set and count the activations in the feedforward layer. Plot the activations as a histogram to see which features in the feedforward layer are activated the most."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
